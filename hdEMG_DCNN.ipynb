{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# load data from mat files\n",
    "# including two variables: EMGs and spikes\n",
    "def load_data_mat(TR, SG = 0, ST = 10, MU = 1, WS = 120, TF = 0, MutiSeg = 0):\n",
    "    # TR - trial name (e.g., 1_30_GM)\n",
    "    # SG - segment ID (e.g., 0-2)\n",
    "    # ST - step size (5, 10, 20, 30, 40, 50)\n",
    "    # MU - motor unit index (0-N, N is the number)\n",
    "    # WS - window size (e.g., 120)\n",
    "    # TF = 0, no shuffle; TF = 1, shuffle;  0<TF<1, seperate data\n",
    "    # MutiSeg - 0: train with one segment of data; 1: train with two segments of data \n",
    "    \n",
    "    seg = [1, 2, 3]\n",
    "    # load train data set\n",
    "    segment = seg[SG]\n",
    "    # construct mat file name based on parameters\n",
    "    prefix = \"{}-SG{}-WS{}-ST{}\".format(TR, segment, WS, ST)\n",
    "    matfile = \"{}.mat\".format(prefix)\n",
    "    if not path.exists(matfile):\n",
    "        pathstr = 'D:\\\\emg_data\\\\'   # data folder for emg \n",
    "        matfile = \"{}{}\".format(pathstr, matfile)\n",
    "        \n",
    "    vnames = ['EMGs', 'Spikes']\n",
    "    # load mat file\n",
    "    data = sio.loadmat(matfile, variable_names=vnames)\n",
    "    x_data = data['EMGs']\n",
    "    spikes = data['Spikes']\n",
    "    \n",
    "    # load second segment if MutiSeg is 1\n",
    "    if MutiSeg:\n",
    "        seg2 = [2, 3, 1]\n",
    "        segment = seg2[SG]\n",
    "        prefix = \"{}-SG{}-WS{}-ST{}\".format(TR, segment, WS, ST)\n",
    "        matfile = \"{}.mat\".format(prefix);  \n",
    "        if not path.exists(matfile):\n",
    "            pathstr = 'D:\\\\emg_data\\\\'\n",
    "            matfile = \"{}{}\".format(pathstr, matfile)\n",
    "    #     print(matfile)\n",
    "        data_2 = sio.loadmat(matfile, variable_names=vnames)\n",
    "        x_data_2 = data_2['EMGs']\n",
    "        spikes_2 = data_2['Spikes']\n",
    "        x_data = np.concatenate((x_data, x_data_2)) \n",
    "        spikes = np.concatenate((spikes, spikes_2)) \n",
    "\n",
    "#     x_data.shape\n",
    "    # exactract spikes for given motor units\n",
    "    if type(MU) is list:\n",
    "        y_data = []\n",
    "        for c in MU:\n",
    "            if c < spikes.shape[1]:\n",
    "                y_data.append(spikes[:, c])\n",
    "            else:\n",
    "                y_data.append(spikes[:, -1]*0)\n",
    "    else:\n",
    "        y_data = []\n",
    "        y_data.append(spikes[:, MU])\n",
    "\n",
    "    ## shuffle the data based on TF flag\n",
    "    y_data = np.array(y_data)\n",
    "    y_data = y_data.T\n",
    "    if TF == 1:\n",
    "        x_data, y_data = shuffle(x_data, y_data)\n",
    "    elif TF > 0: \n",
    "        x_data, _, y_data, _= train_test_split(x_data, y_data, test_size = 1.0-TF)\n",
    "    else:\n",
    "        print('no shuffle')\n",
    "    y_data = y_data.T\n",
    "    y_data = list(y_data)\n",
    "\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.metrics import Metric\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "# import neptune\n",
    "\n",
    "# calculate f1 score\n",
    "def f1_m(y_true, y_pred):\n",
    "#     precision = precision_m(y_true, y_pred)\n",
    "#     recall = recall_m(y_true, y_pred)\n",
    "    y_pred_binary = tf.where(y_pred>=0.5, 1., 0.)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred_binary, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred_binary, 0, 1)))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return 2*((precision*recall)/(precision + recall + K.epsilon()))\n",
    "\n",
    "# customized callback function to calculate averaged f1_score and accuracy across all outputs\n",
    "class AccuracyCallback(Callback):\n",
    "    def __init__(self, metric_name = 'accuracy'):\n",
    "        super().__init__()\n",
    "        self.metric_name = metric_name\n",
    "        self.val_metric = []\n",
    "        self.metric = []\n",
    "        self.val_metric_mean = 0\n",
    "        self.metric_mean = 0\n",
    "        self.best_metric = 0\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "#         print('Accuracycallback')\n",
    "        # extract values from logs\n",
    "        self.val_metric = []\n",
    "        self.metric = []\n",
    "        for log_name, log_value in logs.items():\n",
    "            if log_name.find(self.metric_name) != -1:\n",
    "                if log_name.find('val') != -1:\n",
    "                    self.val_metric.append(log_value)\n",
    "                else:\n",
    "                    self.metric.append(log_value)\n",
    "\n",
    "        self.val_metric_mean = np.mean(self.val_metric)\n",
    "        self.metric_mean = np.mean(self.metric)\n",
    "        logs['val_{}'.format(self.metric_name)] = np.mean(self.val_metric)   # replace it with your metrics\n",
    "        logs['{}'.format(self.metric_name)] = np.mean(self.metric)   # replace it with your metrics\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "# import keras\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation, Input\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, BatchNormalization, Dropout, LSTM\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# create convolutional neural network with sequential model\n",
    "def get_cnn1d_model(shape_in, shape_out, nn_nodes = [128, 128, 128, 64, 256]):\n",
    "    '''Create a keras model.'''\n",
    "    # shape_in = (timesteps, features)\n",
    "    model = Sequential()\n",
    "    gg_nn_nodes = nn_nodes\n",
    "    print(gg_nn_nodes)\n",
    "    \n",
    "    #add model layers, number of filter, kernel_size\n",
    "    model.add(Conv1D(filters=gg_nn_nodes[0], kernel_size= 3, activation= 'relu', input_shape=shape_in))\n",
    "    model.add(Conv1D(filters=gg_nn_nodes[1], kernel_size= 3, activation= 'relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv1D(filters=gg_nn_nodes[2], kernel_size=3, activation= 'relu'))\n",
    "    model.add(Conv1D(filters=gg_nn_nodes[3], kernel_size=3, activation= 'relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(gg_nn_nodes[4], activation= 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(shape_out, activation= 'sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "############ create models with given input shape and output shape ##########\n",
    "# create convolutional neural network with API interface\n",
    "def get_cnn1d_api(shape_in, shape_out, nn_nodes = [128, 128, 128, 64, 256]):\n",
    "    '''Create a keras model with functional API'''\n",
    "    # create convolutional neural network model\n",
    "    # shape_in = (timesteps, features)\n",
    "    print(nn_nodes)\n",
    "    \n",
    "    # create shared layers\n",
    "    visible = Input(shape = shape_in, name='EMG')\n",
    "    cnn = Conv1D(filters=nn_nodes[0], kernel_size=3, activation='relu')(visible)\n",
    "    cnn = Conv1D(filters=nn_nodes[1], kernel_size=3, activation='relu')(cnn)\n",
    "    cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "    cnn = Dropout(0.5)(cnn)\n",
    "    \n",
    "    # create seperate layers for each motor unit\n",
    "    outputs = []\n",
    "    for k in range(1, shape_out+1):\n",
    "        cnn_2 = Conv1D(filters=nn_nodes[2], kernel_size=3, activation='relu')(cnn)\n",
    "        cnn_2 = Conv1D(filters=nn_nodes[3], kernel_size=3, activation='relu')(cnn_2)\n",
    "        cnn_2 = MaxPooling1D(pool_size=2)(cnn_2)\n",
    "        cnn_2 = Dropout(0.5)(cnn_2)\n",
    "\n",
    "        cnn_2 = Flatten()(cnn_2)\n",
    "        s2 = Dense(nn_nodes[4], activation='relu')(cnn_2)\n",
    "        s2 = Dropout(0.5)(s2)\n",
    "        output = Dense(1, activation='sigmoid', name='output_{}'.format(k))(s2)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # construct metrics and loss configuration\n",
    "    metrics = {'output_1':['accuracy', f1_m]}\n",
    "    loss = {'output_1':'binary_crossentropy'}\n",
    "    for k in range(2, shape_out+1):\n",
    "        key = 'output_{}'.format(k)\n",
    "        metrics[key] = ['accuracy', f1_m]\n",
    "        loss[key]= 'binary_crossentropy'\n",
    "\n",
    "    # tie together\n",
    "    model = Model(inputs=visible, outputs=outputs)\n",
    "    return model, loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tensorboard for display\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint, LambdaCallback\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# build model with configuration\n",
    "def build_model(WS = 120, n_output = 1, nn_nodes = [128, 128, 128, 64, 256]):\n",
    "    # mIndex is not in use\n",
    "    n_input = WS\n",
    "    n_features = 64 # set default number of EMG channels\n",
    "\n",
    "    if n_output == 1:\n",
    "        print('Sequential model')\n",
    "        model_cnn = get_cnn1d_model((n_input, n_features), n_output, nn_nodes)\n",
    "        loss_cnn = 'binary_crossentropy'\n",
    "        metrics_cnn = [\n",
    "            'accuracy',\n",
    "            'mse',\n",
    "             f1_m,\n",
    "            ]\n",
    "    else:\n",
    "        print('API model')\n",
    "        model_cnn, loss_cnn, metrics_cnn = get_cnn1d_api((n_input, n_features), n_output, nn_nodes)\n",
    "\n",
    "    model = model_cnn\n",
    "    model.compile(optimizer = 'rmsprop', #sgd', 'adagrad', 'rmsprop', 'adam'\n",
    "                    loss = loss_cnn,  # mean_squared_error\n",
    "                    metrics = metrics_cnn) #['accuracy', 'mse'])\n",
    "    return model_cnn\n",
    "\n",
    "\n",
    "def train_model(model, x_data, y_data, prefix, epochs = 100):\n",
    "    tname = int(time.time())\n",
    "    batch_size = 64\n",
    "    \n",
    "    # create tersorboard\n",
    "    log_name = \"hdEMG_{}_{}\".format(prefix, tname)\n",
    "    tensorboard = TensorBoard(log_dir = \".\\\\logs\\\\{}\".format(log_name))\n",
    "    # check tenorboard by running \n",
    "    # tensorboard --logdir C:\\Users\\Yue\\Documents\\mudecomp\\logs\\ # in the anaconda prompt command line\n",
    "    # tensorboard --logdir C:\\Users\\ywen.SMPP\\Documents\\mudecomp\\logs\\\n",
    "    \n",
    "    # early stop when loss improvement is small\n",
    "    es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=50)\n",
    "    \n",
    "    # save the best model when loss is minimum and f1_score is highest\n",
    "    mc = ModelCheckpoint('best_model_{}_{}_l.h5'.format(prefix, tname), monitor='loss', mode='min', verbose=1, save_best_only=True)\n",
    "    mc_vl = ModelCheckpoint( 'best_model_{}_{}_vl.h5'.format(prefix, tname), monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "    mc_f = ModelCheckpoint('best_model_{}_{}_f.h5'.format(prefix, tname), monitor='f1_m', mode='max', verbose=1, save_best_only=True)\n",
    "    mc_vf = ModelCheckpoint('best_model_{}_{}_vf.h5'.format(prefix, tname), monitor='val_f1_m', mode='max', verbose=1, save_best_only=True)\n",
    "    \n",
    "    # create customized callbacks\n",
    "    accuracy_callback = AccuracyCallback('accuracy')\n",
    "    f1_callback = AccuracyCallback('f1_m')\n",
    "    \n",
    "    # train model\n",
    "    history = model.fit(x_data, \n",
    "                        y_data,\n",
    "                        validation_split = 0.2,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        verbose = 1,\n",
    "                        callbacks = [es, mc, mc_vl, accuracy_callback, f1_callback, tensorboard, mc_f, mc_vf])\n",
    "\n",
    "    # return best model for further evaluation\n",
    "    model = load_model(model_name, custom_objects={\"f1_m\": f1_m})\n",
    "    return model, tname\n",
    "\n",
    "# display model structure\n",
    "# pip install pydotplus\n",
    "# pip install pydot\n",
    "# https://bobswift.atlassian.net/wiki/spaces/GVIZ/pages/20971549/How+to+install+Graphviz+software\n",
    "def display_model(model, filename = 'model.png'):\n",
    "    # plot model structure\n",
    "    from tensorflow.keras.utils import plot_model\n",
    "    plot_model(model, to_file='C:\\{}'.format(filename), show_shapes=True)\n",
    "    from IPython.display import Image\n",
    "    Image(filename='C:\\{}'.format(filename))\n",
    "\n",
    "\n",
    "# load model with cuostmized metrics\n",
    "def load_model_custom(model_name):\n",
    "    model = load_model(model_name, custom_objects={\"f1_m\": f1_m})\n",
    "    return model\n",
    "\n",
    "\n",
    "# validate model with given data sets\n",
    "def model_validate(model, x_data, y_data, prefix):\n",
    "    # sequential data\n",
    "    y_pred = evaluate(model, x_data, y_data)\n",
    "    savedata(y_data, y_pred, \"{}\".format(prefix))\n",
    "\n",
    "    \n",
    "# evaluate model prediction\n",
    "def evaluate(model, x_val, y_val, showFlag = 0):\n",
    "    # \n",
    "    print('\\n# Generate predictions')\n",
    "    y_pred = model.predict(x_val)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    if len(y_pred.shape) == 3:\n",
    "        y_pred = np.reshape(y_pred, (y_pred.shape[0], y_pred.shape[1]))\n",
    "    \n",
    "    if showFlag:\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        ax1.plot(y_val)\n",
    "        ax1.set_title('real_value')\n",
    "        ax2.plot(y_pred)\n",
    "        ax2.set_title('predict_value')\n",
    "        plt.show\n",
    "    return y_pred\n",
    "\n",
    "# save prediction and acutal values to csv file\n",
    "def savedata(y_val, y_pred, fname):\n",
    "    \n",
    "    # convert to array if y_val type is list\n",
    "    if type(y_val) is list:\n",
    "        y_val = np.array(y_val)\n",
    "    if type(y_pred) is list:\n",
    "        y_pred = np.array(y_pred)\n",
    "    \n",
    "    # reshape\n",
    "    if len(y_pred.shape) == 3:\n",
    "        y_pred = np.reshape(y_pred, (y_pred.shape[0], y_pred.shape[1]))\n",
    "    if len(y_val.shape) == 3:\n",
    "        y_val = np.reshape(y_val, (y_val.shape[0], y_val.shape[1]))\n",
    "\n",
    "    # rotate\n",
    "    if len(y_val.shape) == 2 and y_val.shape[0] < y_val.shape[1]:\n",
    "        y_val = np.transpose(y_val)\n",
    "    elif len(y_val.shape) == 1:\n",
    "        y_val = np.reshape(y_val, (y_val.shape[0], 1))\n",
    "\n",
    "    if y_pred.shape[0] < y_pred.shape[1]:\n",
    "        y_val = np.transpose(y_val)\n",
    "\n",
    "    # save data\n",
    "    if  y_val.shape[0] > y_val.shape[1] and y_val.shape[0] == y_pred.shape[0]:\n",
    "        data = np.column_stack((y_val, y_pred))\n",
    "#         data = np.transpose(data)\n",
    "    else:\n",
    "        data = np.vstack((y_val, y_pred))\n",
    "    \n",
    "    if data.shape[0] < data.shape[1]:\n",
    "        data = np.transpose(data)\n",
    "    data.shape\n",
    "    pd.DataFrame(data).to_csv(\"output-{}.csv\".format(fname))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
