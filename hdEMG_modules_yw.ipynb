{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import h5py\n",
    "import sys\n",
    "\n",
    "emg_folder_default = 'D:\\\\emg_data\\\\'   # default EMG folder\n",
    "\n",
    "def loadmat(matfile, vnames):\n",
    "    try:\n",
    "        data = sio.loadmat(matfile, variable_names=vnames)\n",
    "        x_data = data[vnames[0]]\n",
    "        y_data = data[vnames[1]]\n",
    "    except NotImplementedError:\n",
    "        Feature=h5py.File(matfile, 'r') #read mat file\n",
    "        x_data = Feature[vnames[0]][:]\n",
    "        y_data = Feature[vnames[1]][:]\n",
    "        x_data = np.transpose(x_data)\n",
    "        y_data = np.transpose(y_data)\n",
    "        print(\"Load h5py\")\n",
    "    except:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        print(matfile)\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "# plot a frame of high-density EMG\n",
    "def plot_frame(frame):\n",
    "    # plot_frame(frame)\n",
    "    plt.figure()\n",
    "    plt.imshow(frame.transpose(), cmap = plt.cm.binary)\n",
    "    plt.show\n",
    "    plt.ylabel('Channels')\n",
    "    plt.xlabel('Time')\n",
    "    plt.savefig('EMG_map.png')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(frame[:,60], 'r', label='ch 60')\n",
    "    plt.plot(frame[:,40], 'b', label='ch 40')\n",
    "    plt.legend(framealpha=1, frameon=True);\n",
    "    plt.show\n",
    "    plt.savefig('EMG_raw.png')\n",
    "\n",
    "# check the number of samples in each class (i.e., number of frames w/wo spikes)\n",
    "def checkData(classes):\n",
    "    if classes.shape[0]==0:\n",
    "        return 0\n",
    "    if ~isinstance(classes, int):\n",
    "        classes = classes.astype(int)\n",
    "    try:\n",
    "        neg, pos = np.bincount(classes)\n",
    "        total = neg + pos\n",
    "        print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "            total, pos, 100 * pos / total))\n",
    "        return 1.0/neg, 1.0/pos\n",
    "    except:\n",
    "        neg = 0\n",
    "        pos = 0\n",
    "        return neg, pos\n",
    "\n",
    "\n",
    "# load data from pickle \n",
    "import pickle\n",
    "def load_data(trial, step_size, ch, seg_index):\n",
    "    seg = [1, 3, 5]\n",
    "    # load train data set\n",
    "    segment = seg[seg_index]\n",
    "    prefix = \"{}_{}_st{}_ch{}\".format(trial, segment, step_size, ch)\n",
    "    x_file = \"{}_x.pickle\".format(prefix)\n",
    "    y_file = \"{}_y.pickle\".format(prefix)\n",
    "    print(prefix)\n",
    "    if not path.exists(x_file):\n",
    "        pathstr = emg_folder_global\n",
    "        x_file = \"{}{}\".format(pathstr, x_file)\n",
    "        y_file = \"{}{}\".format(pathstr, y_file)\n",
    "    else:\n",
    "        print(x_file)\n",
    "    \n",
    "    pickle_in = open(x_file, \"rb\")\n",
    "    x_data = pickle.load(pickle_in)\n",
    "    pickle_in = open(y_file, \"rb\")\n",
    "    y_data = pickle.load(pickle_in)\n",
    "    \n",
    "    checkData(y_data)\n",
    "    plot_frame(x_data[0,:,:])\n",
    "    print(y_data[0])\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "# load data from mat files\n",
    "# including two variables: EMGs and spikes\n",
    "def load_data_mat(TR, SG = 0, ST = 10, MU = 1, WS = 120, TF = 0, MutiSeg = 0, emg_folder_global = None):\n",
    "    # TR - trial name (e.g., 1_30_GM)\n",
    "    # SG - segment ID (e.g., 0-2)\n",
    "    # ST - step size (5, 10, 20, 30, 40, 50)\n",
    "    # MU - motor unit index (0-N, N is the number)\n",
    "    # WS - window size (e.g., 120)\n",
    "    # TF = 0, no shuffle; TF = 1, shuffle;  0<TF<1, seperate data\n",
    "    # MutiSeg - 0: train with one segment of data; 1: train with two segments of data; 2: train with three segments of data \n",
    "    if emg_folder_global is None:\n",
    "        emg_folder_global = emg_folder_default\n",
    "    seg = [1, 2, 3]\n",
    "    # load train data set\n",
    "    segment = seg[SG]\n",
    "    # construct mat file name based on parameters\n",
    "    prefix = \"{}-SG{}-WS{}-ST{}\".format(TR, segment, WS, ST)\n",
    "    matfile = \"{}.mat\".format(prefix)\n",
    "    if not path.exists(matfile):\n",
    "        pathstr = emg_folder_global\n",
    "        matfile = \"{}{}\".format(pathstr, matfile)\n",
    "        \n",
    "    if not path.exists(matfile):\n",
    "        print('{} not exist'.format(matfile))\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        return x_data, y_data\n",
    "    else:\n",
    "        print('{} exist'.format(matfile))\n",
    "\n",
    "    vnames = ['EMGs', 'Spikes_cst']\n",
    "\n",
    "    # load mat file with sio or h5py\n",
    "    x_data, spikes = loadmat(matfile, vnames)\n",
    "    \n",
    "    # load second segment if MutiSeg is 1\n",
    "    if MutiSeg>=1:\n",
    "        seg2 = [2, 3, 1]\n",
    "        segment = seg2[SG]\n",
    "        prefix = \"{}-SG{}-WS{}-ST{}\".format(TR, segment, WS, ST)\n",
    "        matfile = \"{}.mat\".format(prefix);  \n",
    "        if not path.exists(matfile):\n",
    "            pathstr = emg_folder_global\n",
    "            matfile = \"{}{}\".format(pathstr, matfile)\n",
    "    #     print(matfile)\n",
    "        x_data_2, spikes_2 = loadmat(matfile, vnames)\n",
    "        x_data = np.concatenate((x_data, x_data_2)) \n",
    "        spikes = np.concatenate((spikes, spikes_2)) \n",
    "\n",
    "    if MutiSeg>=2:\n",
    "        seg3 = [3, 1, 2]\n",
    "        segment = seg3[SG]\n",
    "        prefix = \"{}-SG{}-WS{}-ST{}\".format(TR, segment, WS, ST)\n",
    "        matfile = \"{}.mat\".format(prefix);  \n",
    "        if not path.exists(matfile):\n",
    "            pathstr = emg_folder_global\n",
    "            matfile = \"{}{}\".format(pathstr, matfile)\n",
    "    #     print(matfile)\n",
    "        x_data_2, spikes_2 = loadmat(matfile, vnames)\n",
    "        x_data = np.concatenate((x_data, x_data_2)) \n",
    "        spikes = np.concatenate((spikes, spikes_2)) \n",
    "\n",
    "    print(x_data.shape)\n",
    "    if MutiSeg>=1:\n",
    "        print('extend')\n",
    "        # expand data\n",
    "        x_data = np.concatenate((x_data, x_data*0.7, x_data*0.4)) \n",
    "        spikes = np.concatenate((spikes, spikes, spikes))\n",
    "    print(x_data.shape)\n",
    "    \n",
    "    # exactract spikes for given motor units\n",
    "    if type(MU) is list:\n",
    "        y_data = []\n",
    "        for c in MU:\n",
    "            if c < spikes.shape[1]:\n",
    "                y_data.append(spikes[:, c])\n",
    "            else:\n",
    "                y_data.append(spikes[:, -1]*0)\n",
    "    else:\n",
    "        y_data = []\n",
    "        y_data.append(spikes[:, MU])\n",
    "\n",
    "    ## shuffle the data based on TF\n",
    "    y_data = np.array(y_data)\n",
    "    y_data = y_data.T\n",
    "    if TF == 1:\n",
    "        x_data, y_data = shuffle(x_data, y_data)\n",
    "    elif TF > 0: \n",
    "        x_data, _, y_data, _= train_test_split(x_data, y_data, test_size = 1.0-TF)\n",
    "    else:\n",
    "        print('no shuffle')\n",
    "    y_data = y_data.T\n",
    "    y_data = list(y_data)\n",
    "    \n",
    "    return x_data, y_data\n",
    "\n",
    "# split data into train set and test set\n",
    "def split_data(x_data, y_data, test_size = 0):\n",
    "    y_data = np.array(y_data)\n",
    "    y_data = y_data.T\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = test_size)\n",
    "\n",
    "    y_train = y_train.T\n",
    "    y_train = list(y_train)\n",
    "    y_test = y_test.T\n",
    "    y_test = list(y_test)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "# shuffle the data\n",
    "def shuffle_data(x_data, y_data):\n",
    "    y_data = np.array(y_data)\n",
    "    y_data = y_data.T\n",
    "    x_data, y_data = shuffle(x_data, y_data)\n",
    "    y_data = y_data.T\n",
    "    y_data = list(y_data)\n",
    "    return x_data, y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.metrics import Metric\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "# import neptune\n",
    "\n",
    "# customized metrics\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "# calculate f1 score\n",
    "def f1_m(y_true, y_pred):\n",
    "    y_pred_binary = tf.where(y_pred>=0.5, 1., 0.)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred_binary, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred_binary, 0, 1)))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return 2*((precision*recall)/(precision + recall + K.epsilon()))\n",
    "\n",
    "\n",
    "# customized callbacks\n",
    "class BaseLogger(Callback):\n",
    "    \"\"\"Callback that accumulates epoch averages of metrics.\n",
    "    This callback is automatically applied to every Keras model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.seen = 0\n",
    "        self.totals = {}\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        batch_size = logs.get('size', 0)\n",
    "        self.seen += batch_size\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            if k in self.totals:\n",
    "                self.totals[k] += v * batch_size\n",
    "            else:\n",
    "                self.totals[k] = v * batch_size\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is not None:\n",
    "            for k in self.params['metrics']:\n",
    "                if k in self.totals:\n",
    "                    # Make value available to next callbacks.\n",
    "                    logs[k] = self.totals[k] / self.seen\n",
    "#         print(logs)\n",
    "\n",
    "\n",
    "class AccuracyCallback(Callback):\n",
    "    def __init__(self, metric_name = 'accuracy'):\n",
    "        super().__init__()\n",
    "        self.metric_name = metric_name\n",
    "        self.val_metric = []\n",
    "        self.metric = []\n",
    "        self.val_metric_mean = 0\n",
    "        self.metric_mean = 0\n",
    "        self.best_metric = 0\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # extract values from logs\n",
    "        self.val_metric = []\n",
    "        self.metric = []\n",
    "        for log_name, log_value in logs.items():\n",
    "            if log_name.find(self.metric_name) != -1:\n",
    "                if log_name.find('val') != -1:\n",
    "                    self.val_metric.append(log_value)\n",
    "                else:\n",
    "                    self.metric.append(log_value)\n",
    "\n",
    "        self.val_metric_mean = np.mean(self.val_metric)\n",
    "        self.metric_mean = np.mean(self.metric)\n",
    "        logs['val_{}'.format(self.metric_name)] = np.mean(self.val_metric)   # replace it with your metrics\n",
    "        logs['{}'.format(self.metric_name)] = np.mean(self.metric)   # replace it with your metrics\n",
    "\n",
    "\n",
    "class updateLogs(Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.val_metric = []\n",
    "        self.metric = []\n",
    "        for log_name, log_value in logs.items():\n",
    "            if log_name.find('f1_m') != -1:\n",
    "#                 print(\"{}:{}\".format(log_name, log_value))\n",
    "                if log_name.find('val') != -1:\n",
    "                    self.val_metric.append(log_value)\n",
    "                else:\n",
    "                    self.metric.append(log_value)   \n",
    "                    \n",
    "        logs['val_f1_m'] = np.mean(self.val_metric)   # replace it with your metrics\n",
    "        logs['f1_m'] = np.mean(self.metric)   # replace it with your metrics\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "# import keras\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation, Input\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, BatchNormalization, Dropout, LSTM, AveragePooling1D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "############ create models with given input shape and output shape ##########\n",
    "gg_nn_nodes = [128, 128, 128, 64, 256]\n",
    "\n",
    "# create convolutional neural network with API interface\n",
    "def get_cnn1d_api(shape_in, shape_out, nn_nodes = [128, 128, 128, 64, 256]):\n",
    "    '''Create a keras model with functional API'''\n",
    "    # create convolutional neural network model\n",
    "    # shape_in = (timesteps, features)\n",
    "#     nn_nodes = [128, 128, 128, 64, 256]\n",
    "#     global gg_nn_nodes\n",
    "    gg_nn_nodes = nn_nodes\n",
    "    print(gg_nn_nodes)\n",
    "    visible = Input(shape = shape_in, name='EMG')\n",
    "    \n",
    "    cnn = Conv1D(filters=gg_nn_nodes[0], kernel_size=3, activation='relu')(visible)\n",
    "    cnn = Conv1D(filters=gg_nn_nodes[1], kernel_size=3, activation='relu')(cnn)\n",
    "    cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "    cnn = Dropout(0.5)(cnn)\n",
    "    \n",
    "    struct_type = 1\n",
    "    if struct_type:\n",
    "        cnn = Conv1D(filters=gg_nn_nodes[2], kernel_size=3, activation='relu')(cnn)\n",
    "        cnn = Conv1D(filters=gg_nn_nodes[3], kernel_size=3, activation='relu')(cnn)\n",
    "        cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "        cnn = Dropout(0.5)(cnn)\n",
    "        cnn_2 = Flatten()(cnn)\n",
    "\n",
    "    outputs = []\n",
    "    for k in range(1, shape_out+1):\n",
    "        if not struct_type:\n",
    "            cnn_2 = Conv1D(filters=gg_nn_nodes[2], kernel_size=3, activation='relu')(cnn)\n",
    "            cnn_2 = Conv1D(filters=gg_nn_nodes[3], kernel_size=3, activation='relu')(cnn_2)\n",
    "            cnn_2 = MaxPooling1D(pool_size=2)(cnn_2)\n",
    "            cnn_2 = Dropout(0.5)(cnn_2)\n",
    "            cnn_2 = Flatten()(cnn_2)\n",
    "\n",
    "        s2 = Dense(gg_nn_nodes[4], activation='relu')(cnn_2)\n",
    "        s2 = Dropout(0.5)(s2)\n",
    "        output = Dense(1, activation='sigmoid', name='output_{}'.format(k))(s2)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    if 0:\n",
    "        k = k + 1\n",
    "        s2 = Dense(nn_nodes[4], activation='relu')(cnn_2)\n",
    "        s2 = Dropout(0.5)(s2)\n",
    "        output_f = Dense(1, activation='sigmoid', name='output_{}'.format(k))(s2)\n",
    "        outputs.append(output_f)\n",
    "    \n",
    "    metrics = {'output_1':['accuracy', f1_m]}\n",
    "    loss = {'output_1':'binary_crossentropy'}\n",
    "    for k in range(2, shape_out+1):\n",
    "        key = 'output_{}'.format(k)\n",
    "        metrics[key] = ['accuracy', f1_m]\n",
    "        loss[key]= 'binary_crossentropy'\n",
    "        \n",
    "    if 0:\n",
    "        k = k + 1\n",
    "        key = 'output_{}'.format(k)\n",
    "        metrics[key] = ['accuracy', 'mse']\n",
    "        loss[key]= 'mse'\n",
    "    \n",
    "    # tie together\n",
    "    model = Model(inputs=visible, outputs=outputs)\n",
    "    return model, loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tensorboard for display\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint, LambdaCallback\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# build model with configuration\n",
    "def build_model(mIndex, WS = 120, n_output = 1, nn_nodes = [128, 128, 128, 64, 256]):\n",
    "    n_input = WS\n",
    "    n_features = 64 # set default number of EMG channels\n",
    "\n",
    "    model_cnn, loss_cnn, metrics_cnn = get_cnn1d_api((n_input, n_features), n_output, nn_nodes)\n",
    "    model = model_cnn\n",
    "\n",
    "#### all possible metrics\n",
    "#     METRICS = [\n",
    "#         keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "#         keras.metrics.MeanSquaredError(name='mse'),\n",
    "#         keras.metrics.Precision(name='precision'),\n",
    "#         keras.metrics.TruePositives(name='tp'),\n",
    "#         keras.metrics.FalsePositives(name='fp'),\n",
    "#         keras.metrics.TrueNegatives(name='tn'),\n",
    "#         keras.metrics.FalseNegatives(name='fn'), \n",
    "#         keras.metrics.Recall(name='recall'),\n",
    "#         keras.metrics.AUC(name='auc'),\n",
    "#     ]\n",
    "\n",
    "    METRICS = [\n",
    "        'accuracy',\n",
    "        'mse',\n",
    "         f1_m,\n",
    "    ]\n",
    "\n",
    "    print(n_output)\n",
    "\n",
    "    if n_output == 1:\n",
    "        model.compile(optimizer = 'rmsprop', #'adam',\n",
    "                        loss = 'binary_crossentropy',\n",
    "                        metrics = METRICS) #['accuracy', 'mse'])\n",
    "    else:\n",
    "        model.compile(optimizer = 'rmsprop', #sgd', 'adagrad', 'rmsprop', 'adam'\n",
    "                        loss = loss_cnn,  # mean_squared_error\n",
    "                        metrics = metrics_cnn) #['accuracy', 'mse'])\n",
    "    return model\n",
    "\n",
    "def train_model(model, x_data, y_data, prefix, epochs = 100):\n",
    "    tname = int(time.time())\n",
    "    batch_size = 64\n",
    "    \n",
    "    # create tersorboard\n",
    "    log_name = \"hdEMG_{}_{}\".format(prefix, tname)\n",
    "    model_name = 'best_model_{}_{}_l.h5'.format(prefix, tname)\n",
    "    model_name_vl = 'best_model_{}_{}_vl.h5'.format(prefix, tname)\n",
    "    model_name_a = 'best_model_{}_{}_a.h5'.format(prefix, tname)\n",
    "    model_name_va = 'best_model_{}_{}_va.h5'.format(prefix, tname)\n",
    "    model_name_f = 'best_model_{}_{}_f.h5'.format(prefix, tname)\n",
    "    tensorboard = TensorBoard(log_dir = \".\\\\logs\\\\{}\".format(log_name))\n",
    "\n",
    "    # early stop when improvement is small\n",
    "    # monitor: val_loss, val_accuracy \n",
    "    es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=50)\n",
    "    \n",
    "    # save the best model when accuracy is the best\n",
    "    mc = ModelCheckpoint(model_name, monitor='loss', mode='min', verbose=1, save_best_only=True)\n",
    "    mc_vl = ModelCheckpoint(model_name_vl, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "    mc_a = ModelCheckpoint(model_name_a, monitor='accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "    mc_va = ModelCheckpoint(model_name_va, monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "    mc_f = ModelCheckpoint('best_model_{}_{}_f.h5'.format(prefix, tname), monitor='f1_m', mode='max', verbose=1, save_best_only=True)\n",
    "    mc_vf = ModelCheckpoint('best_model_{}_{}_vf.h5'.format(prefix, tname), monitor='val_f1_m', mode='max', verbose=1, save_best_only=True)\n",
    "#     custom_save = LambdaCallback(on_epoch_end=saveModel)\n",
    "    \n",
    "    accuracy_callback = AccuracyCallback('accuracy')\n",
    "    f1_callback = AccuracyCallback('f1_m')\n",
    "    \n",
    "    x_train = x_data\n",
    "    y_train = y_data\n",
    "    \n",
    "    # train model\n",
    "    generatorEnable = False\n",
    "    if generatorEnable:\n",
    "#         class_weight = {0 : 1., 1: 1.}\n",
    "        history = model.fit_generator(data_train,  \n",
    "                            steps_per_epoch = num_t/batch_size,\n",
    "                            validation_data = data_val, \n",
    "                            validation_steps = num_v/batch_size,\n",
    "                            epochs = epochs, \n",
    "#                             class_weight=class_weight,\n",
    "                            callbacks=[tensorboard, es, mc])\n",
    "    else:\n",
    "#         class_weight = {0 : 1., 1: 1.}\n",
    "        history = model.fit(x_train, \n",
    "                            y_train,\n",
    "                            validation_split = 0.2,\n",
    "#                             validation_data=(x_valid, y_valid),\n",
    "                            batch_size = batch_size,\n",
    "                            epochs = epochs,\n",
    "                            verbose = 1,\n",
    "#                             class_weight = class_weight,\n",
    "                            callbacks = [es, mc, mc_vl, accuracy_callback, f1_callback, tensorboard, mc_f, mc_vf])\n",
    "    \n",
    "    # return best model for further evaluation\n",
    "    model = load_model(model_name_f, custom_objects={\"f1_m\": f1_m})\n",
    "    return model, tname\n",
    "\n",
    "# display model structure\n",
    "# pip install pydotplus\n",
    "# pip install pydot\n",
    "# https://bobswift.atlassian.net/wiki/spaces/GVIZ/pages/20971549/How+to+install+Graphviz+software\n",
    "def display_model(model, filename = 'model.png'):\n",
    "    # plot model structure\n",
    "    from tensorflow.keras.utils import plot_model\n",
    "    plot_model(model, to_file='C:\\{}'.format(filename), show_shapes=True)\n",
    "    from IPython.display import Image\n",
    "    Image(filename='C:\\{}'.format(filename))\n",
    "    \n",
    "# load model with cuostmized metrics\n",
    "def load_model_custom(model_name, inference = False):\n",
    "    model = load_model(model_name, custom_objects={\"f1_m\": f1_m})\n",
    "    if inference:\n",
    "        model.save('tmp.h5', include_optimizer=False, save_format='h5')\n",
    "        model = load_model('tmp.h5', compile=False)\n",
    "    return model\n",
    "\n",
    "# validate model with given data sets\n",
    "def model_validata(model, x_data, y_data, prefix):\n",
    "    # sequential data\n",
    "    y_pred = evaluate(model, x_data, y_data)\n",
    "    savedata(y_data, y_pred, \"{}\".format(prefix))\n",
    "#     scores = model.evaluate(x_data, y_data, verbose=0)\n",
    "#     print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    \n",
    "# evaluate model prediction\n",
    "def evaluate(model, x_val, y_val, showFlag = 0):\n",
    "    # \n",
    "    print('\\n# Generate predictions')\n",
    "    y_pred = model.predict(x_val)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    if len(y_pred.shape) == 3:\n",
    "        y_pred = np.reshape(y_pred, (y_pred.shape[0], y_pred.shape[1]))\n",
    "    \n",
    "    if showFlag:\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        ax1.plot(y_val)\n",
    "        ax1.set_title('real_value')\n",
    "        ax2.plot(y_pred)\n",
    "        ax2.set_title('predict_value')\n",
    "        plt.show\n",
    "    return y_pred\n",
    "\n",
    "# save prediction and acutal values to csv file\n",
    "def savedata(y_val, y_pred, fname):\n",
    "\n",
    "    if type(y_val) is list:\n",
    "        y_val = np.array(y_val)\n",
    "    if type(y_pred) is list:\n",
    "        y_pred = np.array(y_pred)\n",
    "        \n",
    "    if len(y_pred.shape) == 3:\n",
    "        y_pred = np.reshape(y_pred, (y_pred.shape[0], y_pred.shape[1]))\n",
    "        \n",
    "    if len(y_val.shape) == 3:\n",
    "        y_val = np.reshape(y_val, (y_val.shape[0], y_val.shape[1]))\n",
    "\n",
    "    if len(y_val.shape) == 2 and y_val.shape[0] < y_val.shape[1]:\n",
    "        y_val = np.transpose(y_val)\n",
    "    elif len(y_val.shape) == 1:\n",
    "        y_val = np.reshape(y_val, (y_val.shape[0], 1))\n",
    "        \n",
    "    if y_pred.shape[0] < y_pred.shape[1]:\n",
    "        y_val = np.transpose(y_val)\n",
    "           \n",
    "    # save data\n",
    "    if  y_val.shape[0] > y_val.shape[1] and y_val.shape[0] == y_pred.shape[0]:\n",
    "        data = np.column_stack((y_val, y_pred))\n",
    "#         data = np.transpose(data)\n",
    "    else:\n",
    "        data = np.vstack((y_val, y_pred))\n",
    "    \n",
    "#     if data.shape[0] < data.shape[1]:\n",
    "    data = np.transpose(data)\n",
    "\n",
    "    data.shape\n",
    "    pd.DataFrame(data).to_csv(\"output-{}.csv\".format(fname))\n",
    "\n",
    "\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "def plot_metrics(history):\n",
    "    # history = model.fit(...)\n",
    "    # plot_metrics(history)\n",
    "    metrics =  ['loss', 'auc', 'precision', 'recall']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(2,2,n+1)\n",
    "        plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_' + metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        if metric == 'loss':\n",
    "            plt.ylim([0, plt.ylim()[1]])\n",
    "        elif metric == 'auc':\n",
    "            plt.ylim([0.8,1])\n",
    "        else:\n",
    "            plt.ylim([0,1])\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "# not used anymore\n",
    "def crossValidate(model, trialName, segMax = 3, chMax = 8):\n",
    "    # crossValidate(model, '1_50_GM')\n",
    "    modelType = [\"nn\", \"cnn\", \"rnn\"]\n",
    "    trial = trialName\n",
    "    modelIndex = 1\n",
    "    step_size = 5\n",
    "    tname = int(time.time())\n",
    "    subFolder = model.yname[:-3]\n",
    "#     print(model.yname)\n",
    "    for ch in range(0, chMax):\n",
    "        for seg in range(0, segMax):\n",
    "            prefix = \"{}-{}-ST{}-CH{}\".format(trial, modelType[modelIndex], step_size, ch)\n",
    "            # x_test, y_test = load_data(trial, step_size, ch, seg)\n",
    "            x_test, y_test = load_data_mat(TR = trial, SG = seg, ST = step_size, CH = ch)\n",
    "            prefix4file = \"{}-SG{}-T{}\".format(prefix, seg, tname)\n",
    "            print(prefix4file)\n",
    "            os.chdir(subFolder) \n",
    "            model_validata(model, x_test, y_test, prefix4file)\n",
    "            os.chdir('..')\n",
    "        \n",
    "# evaluate model with data generator - not used anymore\n",
    "def evaluate_gen(model, data_set):\n",
    "    # data_set is generator\n",
    "    predict = []\n",
    "    realValue = []\n",
    "    evaluate_gen = data_set\n",
    "    print('Samples: %d' % len(evaluate_gen))\n",
    "    for i in range(len(evaluate_gen)):\n",
    "        x, y = evaluate_gen[i]\n",
    "        yhat = model.predict(x)\n",
    "        yhat = np.hstack(yhat)\n",
    "        print('.', end = '')\n",
    "        #print(y)        #print(yhat)        #print(yhat-y)\n",
    "        #print(yhat[numpy.where(y==1)[0]])\n",
    "        predict.extend(yhat)\n",
    "        realValue.extend(y)\n",
    "    \n",
    "    # Creates two subplots and unpacks the output array immediately\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.plot(realValue)\n",
    "    ax1.set_title('real_value')\n",
    "    ax2.plot(predict)\n",
    "    ax2.set_title('predict_value')\n",
    "    plt.show\n",
    "\n",
    "    # save data\n",
    "    data = np.vstack((realValue, predict))\n",
    "    data = np.transpose(data)\n",
    "    data.shape\n",
    "    pd.DataFrame(data).to_csv(\"output_train_cnn.csv\")\n",
    "\n",
    "# not used in this study\n",
    "# best_val_acc = 0\n",
    "# best_val_loss = sys.float_info.max \n",
    "def saveModel(epoch,logs):\n",
    "    val_acc = logs['val_acc']\n",
    "    val_loss = logs['val_loss']\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        model.save(...)\n",
    "    elif val_acc == best_val_acc:\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss=val_loss\n",
    "            model.save(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
